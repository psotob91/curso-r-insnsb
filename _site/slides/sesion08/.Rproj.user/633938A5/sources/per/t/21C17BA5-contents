---
title: "<img data-src='images/logo-essalud.png' height='72' width='250'/> <img data-src='images/logo-pueblo.jpg' height='72' width='250'/> <img data-src='images/logo-ietsi.png' height='72' width='250'/> <FONT color='#232429'><br>Sesión 4</FONT>"
subtitle: "<FONT color='#636363' size='7'>Programa de Formación Científica:<br>Análisis Estadístico 2022</FONT>"
author: "<FONT color='#232429' size='30'>Percy Soto-Becerra</FONT>"
institute: "<FONT color='#232429' size='5'>Instituto de Evaluación de Tecnologías en Salud e Investigación - IETSI, EsSalud<br>@github/psotob91</FONT>"
date: "<FONT color='#232429' size='6'>Julio 1, 2022</FONT>"
format: 
  revealjs: 
    theme: default
    footer: "Programa de Formación Científica: Análisis Estadístico 2022 - Sesión 4"
    logo: images/logo-ietsi.png
    transition: convex
    background-transition: zoom
    incremental: false
    slide-number: true
    preview-links: true
    # parallax-background-image: images/bg-ietsi-slide-first.png
    # parallax-background-size: "1920px 1080px"
    chalkboard: true
    code-block-background: true
    code-block-border-left: "#31BAE9"
    highlight-style: solarized
    echo: true
    multiplex: true
    touch: true
    auto-stretch: true
    link-external-icon: true
    link-external-newwindow: true
---

```{r}
#| echo: false
#| output: false

# Removing all objects including loaded libraries
rm(list = ls(all = TRUE))
gc()

# Installing and loading packages
if (!require("pacman")) {
  install.packages("pacman")
}

pacman::p_unload("all") # Unloading all package except base

pacman::p_load(tidyverse, 
               janitor, 
               gt, 
               gtsummary, 
               flextable, 
               kableExtra, 
               skimr, 
               Hmisc, 
               readxl, 
               rstatix, 
               ggpubr, 
               car, 
               performance,
               see, 
               lmtest, 
               sandwich, 
               splines2) # Loading packages

# icons::download_fontawesome()
```

# Modelo de Regresión Lineal

## Análisis de regresión

![](images/regresion1.png)

## Modelos de regresión multivariable

![](images/regresion2.png)

## ¿Para qué usamos los modelos de regresión?

-   Según `STRATOS` podemos usar regresión para 3 propósitos diferentes:

    -   <FONT size='6'>Descripción\*</FONT>

    -   <FONT size='6'>Predicción</FONT>

    -   <FONT size='6'>Explicación</FONT>

## Propósitos del modelamiento

![](images/modelos1.png)

::: aside
<br> <FONT size='4'>Clasificación inspirado en: Miguel A. Hernán, John Hsu & Brian Healy (2019) A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks, CHANCE, 32:1, 42-49, DOI: 10.1080/09332480.2019.1579578</FONT>
:::

## Propósitos del modelamiento (cont.)

![](images/modelos2.png)

::: aside
<br> <FONT size='4'>Clasificación inspirado en: Miguel A. Hernán, John Hsu & Brian Healy (2019) A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks, CHANCE, 32:1, 42-49, DOI: 10.1080/09332480.2019.1579578</FONT>
:::

## ¿Para qué usamos los modelos de regresión? (cont.)

-   Este curso se centrará solamente en algunas aplicaciones.

-   Regresión para descripción:

    -   "Factores asociados..:" No necesariamente importa que los factores sean causales.
    -   Evaluación de la magnitud de desigualdades, magnitud de brechas, etc.

## ¿Para qué usamos los modelos de regresión? (cont.) {.scrollable}

-   Regresión para explicación:

    -   "Efecto / Efectividad / Impacto": Busca estimar efectos causales.
    -   Explorar potenciales factores causales... (puede clasificarse dentro de descripción)

-   Regresión para predicción:

    -   Factores pronóstico o predictores de...": Identifican predictores de interés que luego alimenten mdelos predictivos.
    -   Modelos de predicción: Predicción para diagnóstico y pronóstico.

## ¿Para qué usamos los modelos de regresión? (cont.)

-   No abordaremos modelos de regresión para desarrollar modelos o reglas de predicción clínica.

-   Tampoco para métodos de inferencia causal robusta.

## Regresión Lineal

-   Método estadístico que modela la `relación` entre una `variable continua (dependiente)` y otras `variables (independientes)`.

![](images/regresion-lineal.png)

## Relación entre dos variables

::: columns
::: {.column width="40%"}
-   $Y$ es `variable resultado` (*outcome*), respuesta o dependiente.

-   $X$ es una `variable explicativa`, predictora o regresora.

-   En la figura, a mayor valor de $X$, mayor valor de $Y$.

```{r}
#| echo: false
set.seed(123)
x = rnorm(500, 0, 1)
y = 4 * x + rnorm(50, 0, 2)

datos <- data.frame(
  x = x, 
  y = y)
```
:::

::: {.column width="60%"}
```{r}
#| echo: false
#| fig-align: center
#| fig-width: 7
#| fig-height: 7
 
datos %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  labs(x = "X", y = "Y") + 
  theme_bw()
```
:::
:::

## ¿Cómo podemos resumir la relación entre ambas variables?

::: columns
::: {.column width="50%"}
-   Podemos tratar de dibujar una `línea recta` que `resuma` la relación.

-   Existen `infinitas rectas posibles` que podríamos trazar: ¿Cuál elegir?
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-width: 10
#| fig-height: 10
 
datos %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  geom_abline(aes(x = x, y = y), intercept = -1, slope = 1, colour = "red") + 
  geom_abline(aes(x = x, y = y), intercept = 0.07, slope = 4.4 , colour = "green") + 
    geom_abline(aes(x = x, y = y), intercept = 0.07, slope = 3.9 , colour = "orange") + 
  labs(x = "X", y = "Y") + 
  theme_bw()
```
:::
:::

## ¿Cómo podemos resumir la relación entre ambas variables? (cont.)

::: columns
::: {.column width="50%"}
-   Una opción sería elegir una `recta` que pase por el `valor más representativo` del $y_i$ en cada valor fijo de $x_1$.
    -   Una `recta` que `conecte` los `promedios condicionados` en $x_1$
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-width: 10
#| fig-height: 10
 
datos %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE) + 
  labs(x = "X", y = "Y") + 
  theme_bw()
```
:::
:::

## Anatomía de la RLS {.scrollable}

-   Entonces, la `recta que conecta los promedios` de $y_i$ `condiciondos` en $x_{1i}$ se puede expresar mediante la siguiente `combinación lineal`:

$$\beta_0 + \beta_1x_{1i}$$

-   **Componente Sistemático:** Formalmente hablando, para cada observación $i$ en la población, podemos `relacionar` el `valor esperado` (promedio) $E[y_i]$ de $y_i$ (también llamado $\mu_i$) con la `variable explicativa` $x_{1i}$ mediante la siguiente `ecuación lineal`:

$$E[Y | X_1 = x_{1i}] = E[y_i] =  \mu_i = \beta_0 + \beta_1x_{1i}$$

-   Donde:
    -   $y_i$ son `variables aleatorias` independientes e idénticamente distribuidas (`i.i.d`)
    -   $x_1$ es una variable cuyas valores son fijos y conocidos: $x_1i$:
        -   Se asume se `miden sin error`.
        -   `No importa` su `distribución`.
    -   $\beta_0$ y $\beta_1$ son `parámetros desconocidos` de una superpoblación infinita.
        -   Llamados `coeficientes de regresión` y son una `medida de asociación`.
        -   Es lo que `queremos estimar` con los datos de la muestra!

## Anatomía de la RLS (cont.) {.scrollable}

-   Notar que el `componente sistemático` solo `relaciona` el `promedio condicionado` de $y_i$ con las `variables explicativas`, NO con los valores individuales.

    -   Esta es una manera de obtener una medida que resuma las relaciones individuales en una sola medida.

-   **Componente aleatorio:** Para poder relacionar completamente los valores individuales se agrega un término de error $\epsilon$, el cual se obtiene de restar el valor observado $y_i$ con el valor esperado de este ($\mu_i$):

$$\epsilon_i = y_i - \mu_i$$

-   El problema es que el término de error $\epsilon_i$ no puede predecirse ni estimarse con los datos, se considera que es el componente no explicado por estos.

    -   Para lidiar con este, se asume que su comportamiento puede predecirse a nivel probabilístico: Se asume una distribución de este.
    -   El error $\epsilon_i$ hereda la distribución de probabilidad de $y_i$.

-   Por lo tanto, el valor individual de cada $y_i$ puede ser denotado por la siguiente expresión:

$$y_i = \beta_0 + \beta_1x_{1i} + \epsilon_i$$

-   Para hacer inferencia estadística, a menudo se asume lo siguiente:

$$y_i \sim N(\beta_0 + \beta_1x_{1i}, \sigma^2)$$

$$\epsilon_i \sim N(0, \sigma^2) $$ \## Regresión Lineal Normal

![](images/regresion-normal.png)

## 

::: callout-note
### Algunas notas sobre normalidad

-   No es necesario que $\epsilon_i$ o $y_i$ sigan una distribución normal para que los coeficientes de regresión $\beta$ puedan estimarse de manera puntual.

-   Sin embargo, para estimar el `valor p` o los `intervalos de confianza` mediante `inferencia clásica` sí se necesita asumir una distribución conocida. El modelo de regresión lineal normal asume normalidad de estos.

    -   Asimismo, el modelo es robusto a desviaciones leves/moderadas de la normalidad cuando se cumple el TLC (número de observaciones grande).

-   Otros enfoques para inferencia flexibilizan este supuesto: p. ej., bootstrap, varianza robusta, modelo lineal generalizado que asume otras distribuciones, etc.
:::

## Estimación de ecuación de regresión

-   En la práctica no conocemos los valores de los parámetros, así que los estimamos de nuestros datos.

![](images/regresion-parametro-estim.png)

## ¿Cómo estimamos la ecuación lineal que mejor ajusta a los datos observados?

-   Usamos métodos numéricos:

    -   Método de Mínimos Cuadrados Ordinarios

    -   Método de Máxima Verosimilitud

-   Ambos métodos son equivalentes para el caso de la regresión lineal normal.

## Regresión Lineal Simple sobre variable explicativa categórica

-   Las variables categóricas no son continuas, en cambio son discretas y asumen unos cuantos valores.

-   ¿Cómo estimar una medida de asociación cuando la variable explicativa es categórica?

```{r}
#| echo: false
set.seed(123)
x <- c(rep(0, 25), rep(1, 25))
y <- 4 * x + rnorm(50, 0, 2)

datos <- data.frame(
  x1 = factor(x, levels = c(0, 1), labels = c("Placebo", "Tratamiento Nuevo")), 
  x2 = x, 
  y = y)
```

## Regresión Lineal Simple sobre variable explicativa categórica

```{r}
#| echo: false
datos %>% 
  ggplot(aes(x = x1, y = y)) + 
  geom_point() + 
  theme_bw()
```

## Regresión Lineal Simple sobre variable explicativa categórica (cont.)

-   Si la variable es binaria, una forma de abordar el análisis es asignando a una categoría el valor de 1 y a otra el valor de 0.

    -   Entonces, asumiremos que la variable categórica es numérica para los efectos de todo cálculo.

    -   Sin embargo, la interpretación se centrará en la comparación de categorías.

## Regresión Lineal Simple sobre variable explicativa categórica (cont.)

```{r}
#| echo: false
datos %>% 
  ggplot(aes(x = x2, y = y)) +   
  geom_smooth(method = "lm", se = FALSE) + 
  geom_point() + 
  theme_bw()
```

## Regresión Lineal Simple en R {.scrollable}

```{r}
#| echo: false
set.seed(123)
x <- c(rep(0, 25), rep(1, 25))
z <- rnorm(50, 15, 1)
y <- 10 * x - 2 * z + rnorm(50, 0, 2)

datos1 <- data.frame(
  x1 = factor(x, levels = c(0, 1), labels = c("Placebo", "Tratamiento Nuevo")), 
  x2 = x, 
  x3 = z, 
  y = y)
```

-   Se usa la función `lm()` de R base. Sin embargo, la salida de esta no es muy informativa:

```{r}
lm(y ~ x1, data = datos)
```

-   El modelo puede guardarse para realizar más operaciones sobre este. Por ejemplo, mejorar la salida:

```{r}
mod <- lm(y ~ x1, data = datos)
summary(mod)
```

## Interpretación de salida de RLS {.scrollable}

::: panel-tabset
### Covariable numérica

-   Usamos la función lm():

```{r}
#| echo: false
set.seed(123)
x <- c(rep(0, 25), rep(1, 25))
z <- rnorm(1000, 15, 1)
y <- -10 * x + 1.3 * z + rnorm(100, 0, 2)

datos2 <- data.frame(
  x1_tto = factor(x, levels = c(0, 1), labels = c("Placebo", "Tratamiento Nuevo")), 
  x2_ttonum = x, 
  x3_peso_inicial = z, 
  y_peso_final = y)
```

```{r}
mod <- lm(y_peso_final ~ x3_peso_inicial, data = datos2)
summary(mod)
```

-   El modelo estimado sería el siguiente:

$$y\_pesofinal = -5.4317 + 1.3447*x3\_pesoinicial + \epsilon_i$$

$$\epsilon_i \sim Normal(0, 5.535^2)$$

-   Usando el paquete `{broom}` y su función `tidy()` podemos obtener también los intervalos de confianza:

```{r}
library(broom)
mod %>% 
  tidy(conf.int = TRUE) 
```

-   Interpretación:

    -   $\beta_0$ o `intercepto`: Este viene a ser el valor promedio de $y$ cuando todos los valores de $x$ son 0. En este caso, cuando el peso inicial es cero kg. ¿Esto es posible?, por tal motivo, no se suele interpretar este valor.

    -   $\beta_1$ o coeficiente de regresión de `x3_peso_inicial`: Por `cada 1 kg adicional` de peso inicial, el `valor promedio` del peso final aumenta 1.43 kg (IC95% 1.00 a 1.69; p \< 0.001).

### Covariable categórica

-   Usamos la función lm():

```{r}
mod <- lm(y_peso_final ~ x1_tto, data = datos2)
summary(mod)
```

-   Usando tidy de broom:

```{r}
mod %>% 
  tidy(conf.int = TRUE) 
```

-   Interpretación:

    -   $\beta_0$ (Intercept): A menudo no se interpreta. Es el valor promedio de $y_i$ cuando los valores de $x$ son cero. En este caso, cuando el tratamien es cero (placebo). ¿Esto es posible?, sí es posible pero no es de ayuda para modelos explicativos, por lo que no se interpreta.

    -   $\beta1$ x1Tratamiento Nuevo: El promedio de peso final en quienes recibieron el tratamiento nuevo fue 10.23 kg menor que el de quienes recibieron placebo (Dif. medias = -10.23; IC95% -10.54 a -9.92; p \< 0.001).
:::

## Regresión Lineal Múltiple {.scrollable}

-   Generaliza la RLS permitiendo evaluar la relación de varias covariables explicativas $x$ sobre $y_i$.

-   Para $p$ variables explicativas, el modelo puede expresarse como:

**Componente sistemático:**

$$E[Y | X_1 = x_{1i}, ..., X_p = x_{pi}] = E[y_i] =  \mu_i = \beta_0 + \beta_1x_{1i} + ... + \beta_px_{pi}$$

**Componente aleatoria:**

$$y_i \sim N(\beta_0 + \beta_1x_{1i} + ... + \beta_px_{pi}, \sigma^2)$$

$$\epsilon_i \sim N(0, \sigma^2) $$

## Regresión Lineal en gráficos {.scrollable}

::: panel-tabset
### RLS

::: columns
::: {.column width="40%"}
-   La ecuación de la RLS representa una línea recta.
:::

::: {.column width="60%"}
![](images/regresion-linea.png)
:::
:::

### RLM con 2 X

::: columns
::: {.column width="40%"}
-   La ecuación de la RLM con dos variables explicativas ya no representa una línea recta, sino un plano recto.
:::

::: {.column width="60%"}
![](images/regresion-plano.png)
:::
:::

### RLM con 3 o más X

-   Genera un hiperplano recto.

-   No podemos imaginarnos una imagen de esto, pero sí podemos analizarlo a nivel estadístico.
:::

## RLM en R {.scrollable}

-   Usamos la función lm():

```{r}
#| echo: false
set.seed(123)
x <- c(rep(0, 25), rep(1, 25))
z <- rnorm(1000, 15, 1)
y <- -10 * x + 1.3 * z + rnorm(100, 0, 2)

datos2 <- data.frame(
  x1_tto = factor(x, levels = c(0, 1), labels = c("Placebo", "Tratamiento Nuevo")), 
  x2_ttonum = x, 
  x3_peso_inicial = z, 
  y_peso_final = y)
```

```{r}
mod <- lm(y_peso_final ~ x1_tto + x3_peso_inicial, data = datos2)
summary(mod)
```

-   El modelo estimado sería el siguiente:

$$y\_pesofinal = -0.94719 -10.25530*x1ttoTratamientoNuevo + 1.3875*x3\_pesoinicial + \epsilon_i$$

$$\epsilon_i \sim Normal(0, 2.073^2)$$

-   Usando el paquete `{broom}` y su función `tidy()` podemos obtener también los intervalos de confianza:

```{r}
library(broom)
mod %>% 
  tidy(conf.int = TRUE) 
```

-   Interpretación:

    -   $\beta_0$ o `intercepto`: Este viene a ser el valor promedio de $y$ cuando todos los valores de $x$ son 0. En este caso, cuando el peso inicial es cero kg y cuando el tratamiento es placebo. ¿Esto es posible?, por tal motivo, no se suele interpretar este valor.

    -   $\beta_2$ o coeficiente de regresión de `x1_ttoTratamiento Nuevo`: El promedio de peso final en quienes recibieron el tratamiento nuevo fue 10.26 kg menor que el de quienes recibieron placebo, luego de ajustar por peso inicial (Dif. medias = -10.26; IC95% -10.51 a -9.99; p \< 0.001).

    -   $\beta_1$ o coeficiente de regresión de `x3_peso_inicial`: Por `cada 1 kg adicional` de peso inicial, el `valor promedio` del peso final aumenta 1.39 kg, luego de ajustar por tatamiento recibido (IC95% 1.26 a 1.52; p \< 0.001).

:::

## Errores y residuos

-   Los `errores` ($\epsilon_i$) son medidas de la población a la que no tenemos acceso.

    -   Sin embargo, varios supuestos de la regresión involucran a los errores inaccesibles por el investigador.

-   Los `residuos` ($e_i$) son el análogo a los `errores` pero obtenidos de la `muestra observada`.

-   Podemos usar los `residuos` para `evaluar` algunos `supuestos` sobre los `errores`.

## Residuos gráficamente

![](images/recta-residuo.png)

## Supuestos de la regresión lineal normal

-   Linealidad

-   Independencia de observaciones

-   Homocedasticidad de los errores $\epsilon_i$

-   Normalidad de los errores $\epsilon_i$ o de $y_i$.

-   No problemas con la regresión:

    -   Puntos influyentes.
    -   (Multi) colinealidad: Solo cuando es un problema, no siempre lo es.

##  {.scrollable}

::: callout-note
### Algunas notas sobre los errores y residuos

-   En realidad, los supuestos de los modelos lineales son sobre el comportamiento probabilístico de $y_i$.

-   Sin embargo, la idea de la existencia de los `errores` y de sus valores observados en la muestra, `residuos` resulta útil para evaluar supuestos.

    -   Permiten reducir un problema de muchas dimensiones a solo 1 o 2 dimensiones.

    -   Son como las placas radiográficas para el diagnóstico de los modelos.

![](images/residuos-placa.png)
:::

## ¿Cómo evaluar los supuestos de la regresión lineal? {.scrollable}

-   Se usan los residuos para explorar el comportamiento de los $y_i$ o los errores $\epsilon$.

-   Preferiblemente usar gráficos de residuos.

    -   Pruebas de hipótesis que usan residuos tienen los mismos problemas que discutimos en clases anteriores.

    -   Podríamos usarlas para complementar análisis cuando los tamaños de muestra no son ni muy pequeños ni muy grandes.

-   La función `check_model` del paquete `{performance}` genera un panel de gráficos muy útil para evalur estos supuestos.

-   Podemos complentar el análisis de supuestos con funciones del paquete `{car}`.

::: panel-tabset
### Panel general

```{r}
#| fig-width: 10
#| fig-height: 15
library(performance)
check_model(mod)
```

### Lin. det.

-   Podemos usar gráficos de residuos parciales + Componente:

```{r}
library(car)
crPlots(mod)
```

-   También podemos usar gráficos de variable agregada

```{r}
avPlots(mod)
```

### Homo. det.

-   Se puede evaluar si la homocedasticidad es consistente según cada variable predictora.

-   Si no lo es, se puede optar por modelar esta heterogeneidad de varianzas.

-   Se sugiere usar `residuos estudentizados`.

```{r}
residualPlots(mod, type = "rstudent")
```

### P. inf. det.

-   En el caso de modelos explicativos, importa determinar si hay un impacto en los coeficientes de regresion.

-   Los `dfbetas` pueden ser útiles para evaluar esto:

```{r}
dfbetasPlots(model = mod, id.n = 5)
```

-   Otras medidas también pueden evaluarse:

```{r}
#| fig-width: 10
#| fig-height: 15
influenceIndexPlot(model = mod, id.n = 5)
```
:::

## ¿Cómo flexibilizar supuestos? {.scrollable}

::: panel-tabset
### No linealidad

-   El supuesto de linealidad es sobre los coeficientes de regresión $\beta$, no sobre las covariables.

-   Las variables X deben estar en una forma apropiada para que el supuesto se cumpla.

-   Es bien difícil que exista linealidad en la realida, pero puede ocurrir en raras y excepcionales ocasiones.

    -   Sobre todo cuando la variable está acotada en valores donde la linealidad es plausible.

-   Se sugiere asumir no linealidad y pre-planear un modelamiento no lineal.

-   Entre los métodos que pueden usarse, tenemos:

    -   `Splines`: Bastante usado y sugerido en bioestadística. Útil para ajustar por variables continuas.

    -   `Modelamiento Multivariablede polinomios fraccionales`. También usado y recomendado en literatura biomédica. Útil para modelar forma como objetivo principal.

    -   `Polinomios`. Menos flexible, puede ser útil si se conoce bien la relación o se busca mejorar ajuste.

    -   `Modelos aditivos generalizados`. Útil si se buscar modelar la relación. Complejos y requieren muchos datos.

-   Veamos un ejemplo de modelamiento continuo con `splines`:

::: callout-note
## Evite categorizar la variable continua

-   Categorizar es muy malo: se pierde información y se corre el riesgo de sesgar resultados.

-   Si se quiere ajustar por variables continuas, use Splines o Polinomios fraccionales. No requiere interpretar sus resultados, pero si ajsutar bien!

-   Si se quiere evaluar la relación de la variable continua, planee un método estadístico para modelar la forma sin asumir linealidad.

    -   Presuponga que la relación no es lineal.

    -   Modelo y responda su pregunta. Si la relación es lineal, el modelo más complejo revelerá una línea recta.
:::

### Heterocedasticidad

-   No homogeneidad de varianzas

-   Podemos usar una estimación robusta de la varianza.

-   Los paquetes `{sanwich}` y `{lmtest}` proporcionan funciones útiles para esto.

-   Es bien difícil de creer que existe homogeneidad de varianzas en la vida real (salvo muy raras y excepcionales ocasiones).

    -   Se sugiere planear el proyecto asumiendo que no hay homocedasticidad y usar inferencia robusta de manera pre-planeada.

```{r}
library(lmtest)
library(sandwich)
coeftest(mod, vcov = vcovHC) %>% 
  tidy(conf.int = TRUE)
```

### No normalidad

-   Si distribución es normal (cosa que no podemos saber con certeza), podemos dejar de preocuparnos por este supuesto.

-   Si se cumple TLC, podemos dejar de preocuparnos por este supuesto.

-   Si no se cumple TLC o hay dudas razonables, podemos optar por alguna de las siguientes alternativas:

    -   Transformar Y para normalizar (p. ej., logaritmo)
    -   Usar varianza robusta
    -   Estimar varianza con bootstrapping u otro método de remuestreo.
:::

## Tablas de regresión lineal reproducible {.scrollable}

-   Podemos usar la librería {gtsummary} para esto.

-   Veamos un ejemplo.

```{r}
library(haven)
datos <- read_dta("hb.dta") %>% as_factor()
```

-   Podemos reportar la tabla de regreion multivarible de la siguiente manera:

    -   Primero realizamos el modelo:

```{r}
mod <- lm(hb ~ age + sex, data = datos)
mod %>% 
  tidy(conf.int = TRUE)
```

-   Se puede crear una tabla de regresión multivariable con la función `tbl_regression()` de `{gtsummary}`:

```{r}
tabla_multi <- mod %>%  
  tbl_regression()

tabla_multi
```

-   Podemos hacer la tabla de regresiones bivariada con la función `tbl_uvregression()` de `{gtsummary}`:

```{r}
tabla_univ <- datos %>% 
  select(age, sex, hb) %>% 
  tbl_uvregression(
    method = lm, 
    y = hb
  ) 

tabla_univ
```

-   Luego, podemos fusionar ambas tablas en una sola con la función `tbl_merge()`:

```{r}
tabla_final <- tbl_merge(list(tabla_univ, tabla_multi), tab_spanner = c("Modelos crudos", "Modelo ajustado")) 

tabla_final
```

-   Podemos exportarlo a MS Word para post-procesamiento y reporte:

```{r}
tabla_final %>% 
  as_flex_table() %>% 
  save_as_docx(path = "Tabla_Final.docx")
```

## 

::: r-fit-text
<br>

<center>

slides:

</center>

<center>

[https://bit.ly/3n6Ejzb](https://ietsi-academy-aed.netlify.app/sesiones/sesion-1/sesion-1c)

</center>
:::

## 

::: r-fit-text
<center>

**¡Gracias por su atención!**

</center>

<center>

**¡Encantado de responder sus consultas!**

</center>

<br>

<center>

**Percy Soto-Becerra**

</center>

<br>

<center>

`r icons::fontawesome$brands$twitter` `r icons::fontawesome$brands$github` @psotob91

</center>

<center>

`r icons::fontawesome$solid$inbox` **percys1991\@gmail.com**

</center>
:::

::: aside
<br> <FONT size='4'>Presentación creada vía `Revealjs` en `Quarto` en `RStudio`.</FONT>
:::
